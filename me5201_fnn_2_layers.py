# -*- coding: utf-8 -*-
"""ME5201_FNN_2_layers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H3qq45GAhStr8CLyYiV8KPP_TjXvUX3J
"""

from google.colab import drive
drive.mount('/content/gdrive')

dataset_path = '/content/gdrive/My Drive/ME5201_assignment/Training Dataset ME5201.xlsx'

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load data from Excel
data = pd.read_excel(dataset_path)

# Extract features (X1, X2) and target variable (y)
X = data[['Density of Fluid (kg/m3)', 'Bulk Modulus of Fluid (Pa)']].values
y = data['Sound wave speed in fluid (m/s)'].values

# Split the data into train, validation, and test sets (70-20-10)
num_samples = X.shape[0]
num_train = int(0.7 * num_samples)
num_val = int(0.2 * num_samples)
num_test = num_samples - num_train - num_val

X_train, y_train = X[:num_train], y[:num_train]
X_val, y_val = X[num_train:num_train + num_val], y[num_train:num_train + num_val]
X_test, y_test = X[num_train + num_val:], y[num_train + num_val:]

# Normalize the data (beneficial for neural networks)
mean_X, std_X = np.mean(X_train, axis=0), np.std(X_train, axis=0)
X_train = (X_train - mean_X) / std_X
X_val = (X_val - mean_X) / std_X
X_test = (X_test - mean_X) / std_X

# Define hyperparameters with two hidden layers
input_size = X_train.shape[1]
hidden_layer_sizes = (100, 10)  # Adjust the number of neurons in each layer
output_size = 1
learning_rate = 0.01
epochs = 1000
gradient_clip = 5.0  # Set a threshold for gradient clipping

# Initialize weights and biases for two hidden layers
weights_input_hidden1 = np.random.randn(input_size, hidden_layer_sizes[0])
bias_hidden1 = np.zeros(hidden_layer_sizes[0])
weights_hidden1_hidden2 = np.random.randn(hidden_layer_sizes[0], hidden_layer_sizes[1])
bias_hidden2 = np.zeros(hidden_layer_sizes[1])
weights_hidden2_output = np.random.randn(hidden_layer_sizes[1], output_size)
bias_output = np.zeros(output_size)

# Training loop
for epoch in range(epochs):
    # Forward pass
    hidden_input1 = np.dot(X_train, weights_input_hidden1) + bias_hidden1
    hidden_output1 = np.maximum(0, hidden_input1)  # ReLU activation
    hidden_input2 = np.dot(hidden_output1, weights_hidden1_hidden2) + bias_hidden2
    hidden_output2 = np.maximum(0, hidden_input2)  # ReLU activation
    final_input = np.dot(hidden_output2, weights_hidden2_output) + bias_output
    y_pred = final_input

    # Compute loss (mean squared error)
    loss = np.mean((y_pred - y_train.reshape(-1, 1))**2)

   # Backward pass with two hidden layers
grad_y_pred = 2 * (y_pred - y_train.reshape(-1, 1)) / num_train

# Gradient for the output layer
grad_weights_hidden2_output = np.dot(hidden_output2.T, grad_y_pred)
grad_bias_output = np.sum(grad_y_pred, axis=0)
grad_hidden2_output = np.dot(grad_y_pred, weights_hidden2_output.T)

# Gradient for the second hidden layer
grad_hidden2_input = grad_hidden2_output * (hidden_input2 > 0)  # ReLU gradient
grad_weights_hidden1_hidden2 = np.dot(hidden_output1.T, grad_hidden2_input)
grad_bias_hidden2 = np.sum(grad_hidden2_input, axis=0)

# Gradient for the first hidden layer
grad_hidden1_output = np.dot(grad_hidden2_input, weights_hidden1_hidden2.T)
grad_hidden1_input = grad_hidden1_output * (hidden_input1 > 0)  # ReLU gradient
grad_weights_input_hidden1 = np.dot(X_train.T, grad_hidden1_input)
grad_bias_hidden1 = np.sum(grad_hidden1_input, axis=0)

# Gradient clipping for each weight matrix
grad_norm_hidden2_output = np.linalg.norm(grad_weights_hidden2_output)
if grad_norm_hidden2_output > gradient_clip:
    grad_weights_hidden2_output = gradient_clip * grad_weights_hidden2_output / grad_norm_hidden2_output

grad_norm_hidden1_hidden2 = np.linalg.norm(grad_weights_hidden1_hidden2)
if grad_norm_hidden1_hidden2 > gradient_clip:
    grad_weights_hidden1_hidden2 = gradient_clip * grad_weights_hidden1_hidden2 / grad_norm_hidden1_hidden2

grad_norm_input_hidden1 = np.linalg.norm(grad_weights_input_hidden1)
if grad_norm_input_hidden1 > gradient_clip:
    grad_weights_input_hidden1 = gradient_clip * grad_weights_input_hidden1 / grad_norm_input_hidden1

# Update weights and biases
weights_hidden2_output -= learning_rate * grad_weights_hidden2_output
bias_output -= learning_rate * grad_bias_output
weights_hidden1_hidden2 -= learning_rate * grad_weights_hidden1_hidden2
bias_hidden2 -= learning_rate * grad_bias_hidden2
weights_input_hidden1 -= learning_rate * grad_weights_input_hidden1
bias_hidden1 -= learning_rate * grad_bias_hidden1

# Predictions on validation set with two hidden layers
hidden_input1_val = np.dot(X_val, weights_input_hidden1) + bias_hidden1
hidden_output1_val = np.maximum(0, hidden_input1_val)

hidden_input2_val = np.dot(hidden_output1_val, weights_hidden1_hidden2) + bias_hidden2
hidden_output2_val = np.maximum(0, hidden_input2_val)

final_input_val = np.dot(hidden_output2_val, weights_hidden2_output) + bias_output
y_val_pred = final_input_val.flatten()

# Calculate mean squared error and r2 score for validation set
mse = np.mean((y_val_pred - y_val)**2)
ssr = np.sum((y_val_pred - np.mean(y_val))**2)
sst = np.sum((y_val - np.mean(y_val))**2)
r2 = ssr / sst

print(f'Mean Squared Error on Validation Set: {mse}')
print(f'R^2 Score on Validation Set: {r2}')

# Plot predicted vs actual in 3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X_val[:, 0], X_val[:, 1], y_val, label='Actual')
ax.scatter(X_val[:, 0], X_val[:, 1], y_val_pred, label='Predicted', marker='^')
ax.set_xlabel('Density of Fluid (kg/m3)')
ax.set_ylabel('Bulk Modulus of Fluid (Pa)')
ax.set_zlabel('Sound wave speed in fluid (m/s)')
ax.legend()
plt.show()

# Print the equation for two layers with corrected formatting
print("Equation of the fit:")
print(f"Y = {bias_output[0]:.4f}", end=' ')

# For the first hidden layer
for i in range(input_size):
    weight = weights_input_hidden1[i, 0]
    print(f"+ {weight:.4f} * X{i + 1}", end=' ')
print(f"+ {bias_hidden1[0]:.4f}", end=' ')

# For the second hidden layer
for j in range(hidden_layer_sizes[1]):
    weight = weights_hidden1_hidden2[:, j]
    expression = " + ".join([f'{weight[i]:.4f} * ReLU(hidden_input2_val[:, {i}])' for i in range(len(weight))])
    print(f"+ {expression} + {bias_hidden2[j]:.4f}", end=' ')

# For the output layer
print(f"+ {weights_hidden2_output[0, 0]:.4f} * ReLU(hidden_output2_val) + {bias_output[0]:.4f}")

# Manually check predictions for two layers
def predict_manual(x1, x2):
    # Normalize input (assuming you normalized during training)
    x_normalized = (np.array([x1, x2]) - mean_X) / std_X

    # Forward pass
    hidden_input1 = np.dot(x_normalized, weights_input_hidden1) + bias_hidden1
    hidden_output1 = np.maximum(0, hidden_input1)  # ReLU activation

    hidden_input2 = np.dot(hidden_output1, weights_hidden1_hidden2) + bias_hidden2
    hidden_output2 = np.maximum(0, hidden_input2)  # ReLU activation

    final_input = np.dot(hidden_output2, weights_hidden2_output) + bias_output
    y_pred = final_input.flatten()

    return y_pred[0]

# Get user input for X1 and X2
x1_value = float(input("Enter the value for X1: "))
x2_value = float(input("Enter the value for X2: "))

# Predict Y manually for two layers
predicted_y = predict_manual(x1_value, x2_value)

actual_y = data.loc[(data['Density of Fluid (kg/m3)'] == x1_value) & (data['Bulk Modulus of Fluid (Pa)'] == x2_value), 'Sound wave speed in fluid (m/s)'].values[0]


# Print the results
print(f"Manual Prediction for X1={x1_value}, X2={x2_value}: {predicted_y:.4f}")
print(f"Actual Y from the dataset: {actual_y:.4f}")

